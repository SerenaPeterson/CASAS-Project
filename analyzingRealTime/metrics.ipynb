{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17aa367e33770937"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "702fe589fc0cf9e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TS Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd941fce8a5c97d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"al/preprocessed_old/times_for_static_data.csv\")\n",
    "data = data.drop([\"Place1\", \"Place2\", \"Activity\"], axis=1)\n",
    "data = data.set_index(\"DateTime\")\n",
    "data.drop(columns=data.columns[0], axis=1,  inplace=True)\n",
    "ts_list = {}\n",
    "\n",
    "for person in data.Person.unique():\n",
    "    ts_list[person] = data[data[\"Person\"] == person].drop([\"Person\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "946fc35682a96b11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dynamic Time Warping (DTW)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36274cfd74acb6bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test\n",
    "from dtaidistance import dtw\n",
    "df_array2 = np.random.random((4, 100))\n",
    "dtw.distance_matrix_fast(df_array2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47bb3b7f509cd33f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dtaidistance import dtw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55be4229a3100ba9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "num_features = len(ts_list[1].columns)\n",
    "dist_matrix = np.zeros((num_features, num_features), dtype=np.float32)\n",
    "\n",
    "# for i,feat1 in enumerate(ts_list[1].columns):\n",
    "#     for j, feat2 in enumerate(ts_list[1].columns):\n",
    "#         #distance, path = fastdtw(np.matrix(ts_list[1][\"Sedentary\"].values).T, ts_list[1][feat2].values, dist = euclidean)\n",
    "#         distance, path = fastdtw(transpose[i,:].reshape(-1,1), transpose[j,:].reshape(-1,1), dist = euclidean)\n",
    "#         dist_matrix[i,j] = distance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bba9e434bde1f77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Granger Causality"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46c3e4704e4ad75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_internal_granger_matrix(ts: np.ndarray | pd.DataFrame, maxlag=3) -> np.ndarray | pd.DataFrame:\n",
    "    from statsmodels.tsa.stattools import grangercausalitytests\n",
    "    \"\"\"\n",
    "    Create a Granger causality matrix for all feature pairs within a single time series dataset.\n",
    "    Each cell (i, j) in the matrix represents the Granger causality test result for feature i causing feature j.\n",
    "    Reference: https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.grangercausalitytests.html\n",
    "    \"\"\"\n",
    "    if isinstance(ts, pd.DataFrame):\n",
    "        columns = ts.columns\n",
    "        ts = ts.to_numpy()\n",
    "    else:\n",
    "        columns = None\n",
    "\n",
    "    num_features = ts.shape[1]\n",
    "    causality_matrix = np.zeros((num_features, num_features), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(num_features):\n",
    "            if i != j:\n",
    "                combined_data = np.column_stack((ts[:, i], ts[:, j]))\n",
    "                result = grangercausalitytests(combined_data, maxlag=maxlag, verbose=False)\n",
    "                # We are interested in any causality, so we take the minimum p-value over all lags up to maxlag\n",
    "                # Options of 'ssr_chi2test' and 'params_ftest' are available. I'm not sure which is better.\n",
    "                p_values = [result[lag][0]['ssr_chi2test'][1] for lag in range(1, maxlag + 1)]\n",
    "                causality_matrix[i, j] = np.min(p_values)  # Choose the minimum p-value\n",
    "    if columns is not None:\n",
    "        return pd.DataFrame(causality_matrix, columns=columns, index=columns)\n",
    "    else:\n",
    "        return causality_matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7884bdf25c9bce7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_time_by_granger_causality(df1: pd.DataFrame, df2: pd.DataFrame) -> float:\n",
    "    # you may go crazy here (do whatever)\n",
    "    matrix1 = create_internal_granger_matrix(df1)\n",
    "    matrix2 = create_internal_granger_matrix(df2)\n",
    "    # I chose KLD for fun. There's likely a better summary metric.\n",
    "    return kld(matrix1, matrix2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edd349c3113b9a70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "toy_ts = pd.DataFrame({\"x\": 1, \"y\": np.random.randn(20)})\n",
    "create_internal_granger_matrix(toy_ts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e01bd8c01fda54b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "toy_ts = pd.DataFrame({\"x\": np.random.randn(20), \"y\": np.random.randn(20)})\n",
    "create_internal_granger_matrix(toy_ts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6f75345bb31c4ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing out removal of columns that contain only one value\n",
    "ts1 = list(ts_list.values())[0]\n",
    "df = ts1.loc[:, ts1.nunique() != 1]\n",
    "#create_internal_granger_matrix(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5075b42b11c0f94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = len(ts_list)\n",
    "gc_matrix = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "# Find Granger Causality Matrix\n",
    "for i, person1 in enumerate(ts_list.keys()):\n",
    "    for j, person2 in enumerate(ts_list.keys()):\n",
    "        df1 = ts_list[person1][[\"SecondsCos\",\"SecondsSin\",\"DoyCos\",\"DoySin\",\"Sedentary\"]]\n",
    "        df2 = ts_list[person2][[\"SecondsCos\",\"SecondsSin\",\"DoyCos\",\"DoySin\",\"Sedentary\"]]\n",
    "        gc_matrix[i, j] = compare_time_by_granger_causality(df1,df2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d7dee072e8a07ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# KLD Comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4987ddb2d94d419"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kld(p: pd.DataFrame, q: pd.DataFrame, bins=35) -> float:\n",
    "    from scipy.stats import entropy\n",
    "    \"\"\"\n",
    "    Compute the Kullback-Leibler Divergence using scipy's entropy function. KLD(p||q).\n",
    "    This is an assymetric measure. KLD(p||q) can be understood as the amount of information lost when q is used to approximate p.\n",
    "    So, a lower result implies q is a better approximation of p.\n",
    "    If there's time, I'll partition PDFs by sine/cosine pairs and each categorical feature. \n",
    "    There are pros and cons to partitioning and this holistic approach.\n",
    "    Don't worry about that though, just plop in dataframes and expect a float to return.\n",
    "    \"\"\"\n",
    "    # Estimate PDFs\n",
    "    p_hist = np.histogram(p.to_numpy(), bins, density=True)[0]\n",
    "    q_hist = np.histogram(q.to_numpy(), bins, density=True)[0]\n",
    "\n",
    "    # Avoid division by zero\n",
    "    p_hist[p_hist == 0] = np.finfo(float).eps\n",
    "    q_hist[q_hist == 0] = np.finfo(float).eps\n",
    "\n",
    "    return entropy(p_hist, q_hist, base=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b34c08d2d64ab2de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Initialize matrix\n",
    "n = len(ts_list)\n",
    "kld_matrix = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "# Find KLD for each pair of homes and store in matrix\n",
    "for i, person1 in enumerate(ts_list.keys()):\n",
    "        for j, person2 in enumerate(ts_list.keys()):\n",
    "            kld_matrix[i, j] = kld(ts_list[person1],ts_list[person2])\n",
    "\n",
    "# Save results to file\n",
    "#np.savetxt('results/Matrices/kld_matrix.csv', kld_matrix, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683390b8a54c263e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mutual Information Score\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49333efcbfb2302"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#Initialize matrix\n",
    "n = len(ts_list)\n",
    "mis_matrix = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "# Find KLD for each pair of homes and store in matrix\n",
    "for i, person1 in enumerate(ts_list.keys()):\n",
    "        for j, person2 in enumerate(ts_list.keys()):\n",
    "            mis_matrix[i, j] = metrics.mutual_info_score(ts_list[person1],ts_list[person2])\n",
    "\n",
    "# Only allows for 1D inputs rather than full df; create internal matrix then summarize?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dc186ea0a01f916"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "#np.savetxt('results/Matrices/mis_matrix.csv', mis_matrix, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ef8609a010489c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Earth Mover's Distance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab98bb6a9e3c27df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "#Initialize matrix\n",
    "n = len(ts_list)\n",
    "emd_matrix = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "# Calculate & store\n",
    "for i, person1 in enumerate(ts_list.keys()):\n",
    "        for j, person2 in enumerate(ts_list.keys()):\n",
    "            emd_matrix[i, j] = scipy.stats.wasserstein_distance(ts_list[person1],ts_list[person2])\n",
    "\n",
    "# Only allows for 1D or 2D inputs rather than full df; create internal matrix then summarize?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c176bce7b629b274"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cosine Similarity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdddd4e1f940ff4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = len(ts_list)\n",
    "cosSim_matrix = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "# Find KLD for each pair of homes and store in matrix\n",
    "for i, person1 in enumerate(ts_list.keys()):\n",
    "        for j, person2 in enumerate(ts_list.keys()):\n",
    "            cosSim_matrix[i, j] = metrics.pairwise.cosine_similarity(ts_list[person1],ts_list[person2], dense_output=False)\n",
    "\n",
    "# Save results to file\n",
    "#np.savetxt('results/Matrices/kld_matrix.csv', kld_matrix, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ff6e8bbd973e0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
